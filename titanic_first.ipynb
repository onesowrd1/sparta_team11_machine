{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f6d17aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "613e626c",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = sns.load_dataset('titanic')\n",
    "\n",
    "# 원본 데이터프레임을 복사\n",
    "titanic_original = titanic.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "86f00336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>embarked</th>\n",
       "      <th>class</th>\n",
       "      <th>who</th>\n",
       "      <th>adult_male</th>\n",
       "      <th>deck</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>alive</th>\n",
       "      <th>alone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived  pclass     sex   age  sibsp  parch     fare embarked  class  \\\n",
       "0         0       3    male  22.0      1      0   7.2500        S  Third   \n",
       "1         1       1  female  38.0      1      0  71.2833        C  First   \n",
       "2         1       3  female  26.0      0      0   7.9250        S  Third   \n",
       "3         1       1  female  35.0      1      0  53.1000        S  First   \n",
       "4         0       3    male  35.0      0      0   8.0500        S  Third   \n",
       "\n",
       "     who  adult_male deck  embark_town alive  alone  \n",
       "0    man        True  NaN  Southampton    no  False  \n",
       "1  woman       False    C    Cherbourg   yes  False  \n",
       "2  woman       False  NaN  Southampton   yes   True  \n",
       "3  woman       False    C  Southampton   yes  False  \n",
       "4    man        True  NaN  Southampton    no   True  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "48bbd755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>714.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.383838</td>\n",
       "      <td>2.308642</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0.523008</td>\n",
       "      <td>0.381594</td>\n",
       "      <td>32.204208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.486592</td>\n",
       "      <td>0.836071</td>\n",
       "      <td>14.526497</td>\n",
       "      <td>1.102743</td>\n",
       "      <td>0.806057</td>\n",
       "      <td>49.693429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>20.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.910400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>512.329200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         survived      pclass         age       sibsp       parch        fare\n",
       "count  891.000000  891.000000  714.000000  891.000000  891.000000  891.000000\n",
       "mean     0.383838    2.308642   29.699118    0.523008    0.381594   32.204208\n",
       "std      0.486592    0.836071   14.526497    1.102743    0.806057   49.693429\n",
       "min      0.000000    1.000000    0.420000    0.000000    0.000000    0.000000\n",
       "25%      0.000000    2.000000   20.125000    0.000000    0.000000    7.910400\n",
       "50%      0.000000    3.000000   28.000000    0.000000    0.000000   14.454200\n",
       "75%      1.000000    3.000000   38.000000    1.000000    0.000000   31.000000\n",
       "max      1.000000    3.000000   80.000000    8.000000    6.000000  512.329200"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0e360502",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "survived         0\n",
       "pclass           0\n",
       "sex              0\n",
       "age            177\n",
       "sibsp            0\n",
       "parch            0\n",
       "fare             0\n",
       "embarked         2\n",
       "class            0\n",
       "who              0\n",
       "adult_male       0\n",
       "deck           688\n",
       "embark_town      2\n",
       "alive            0\n",
       "alone            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fe070bf0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_15996\\1733503670.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  titanic['age'].fillna(titanic['age'].median(), inplace=True)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_15996\\1733503670.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  titanic['embarked'].fillna(titanic['embarked'].mode()[0], inplace=True)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_15996\\1733503670.py:9: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  titanic['embark_town'].fillna('Unknown', inplace=True)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_15996\\1733503670.py:16: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  titanic['deck'].fillna('Unknown', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# 나이(age) 열의 결측값을 중앙값으로 채움\n",
    "titanic['age'].fillna(titanic['age'].median(), inplace=True)\n",
    "\n",
    "# 승선한 항구(embarked) 열의 결측값을 최빈값으로 채움\n",
    "titanic['embarked'].fillna(titanic['embarked'].mode()[0], inplace=True)\n",
    "\n",
    "\n",
    "# 승선 도시(embark_town) 열의 결측값을 'Unknown'으로 채움 \n",
    "titanic['embark_town'].fillna('Unknown', inplace=True)\n",
    "\n",
    "\n",
    "# 'deck' 열을 object 타입으로 변환\n",
    "titanic['deck'] = titanic['deck'].astype(str)\n",
    "\n",
    "# 결측값을 'Unknown'으로 채움\n",
    "titanic['deck'].fillna('Unknown', inplace=True)\n",
    "\n",
    "#결측치 제거 데이터 프레임\n",
    "titanic_no_nan = titanic\n",
    "\n",
    "#deck과 embark_town을 fillna(0)로 진행했을 때 TypeError\n",
    "#해결하기 위해 dtype으로 각 데이터 타입을 확인\n",
    "#숫자형 데이터'0'에서 문자형 데이터'Unknown'으로 대체\n",
    "#각 타입을 astype(category)과 (object)로 타입 변화 추가해도 안됨\n",
    "#이유를 찾아보니 원래 카테고리 타입인 데이터에는 새로운 카테고리 값을 추가할 때 에러가 발생\n",
    "#category 타입 경우 새로운 카테고리를 추가하기 위해선 기존 카테고리 목록에 추가 되어있어야 가능 \n",
    "#또는 타입을 변환을 진행할 때 astype(object)로 작성해서 오류\n",
    "#astype(str)로 수정 하니 object타입으로 변환 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "661b9bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 성별 인코딩: 'male'은 0, 'female'은 1로 매핑\n",
    "titanic_no_nan['sex'] = titanic_no_nan['sex'].map(lambda x: 0 if x == 'male' else 1 )\n",
    "# 생존 여부 인코딩: 'yes'는 0, 'no'는 1로 매핑\n",
    "titanic_no_nan['alive'] = titanic_no_nan['alive'].map(lambda x: 0 if x == 'yes' else 1)\n",
    "# 승선한 항구 인코딩: 'C'는 0, 'Q'는 1, 'S'(Southampton)은 2로 매핑\n",
    "titanic_no_nan['embarked'] = titanic_no_nan['embarked'].map(lambda x: 0 if x == 'C' else 1 if x == 'Q' else 2)\n",
    "# 승객 등급 인코딩: 'First'는 0, 'Second'는 1, 'Third'는 2로 매핑\n",
    "titanic_no_nan['class'] = titanic_no_nan['class'].map(lambda x: 0 if x =='First' else 1 if x =='Second' else 2) \n",
    "# 성별에 따른 그룹화 인코딩: 'man'은 0, 나머지(여성)는 1로 매핑\n",
    "titanic_no_nan['who'] = titanic_no_nan['who'].map(lambda x: 0 if x == 'man' else 1)\n",
    "# 성인 남성 여부 인코딩: 'False'는 0, 'True'는 1로 매핑\n",
    "titanic_no_nan['adult_male'] = titanic_no_nan['adult_male'].map(lambda x: 0 if x == 'False' else 1)\n",
    "# 갑판 인코딩: 'A'는 0, 'B'는 1, 'C'는 2, 'D'는 3, 'E'는 4, 'F'는 5, 'G'는 6으로 매핑\n",
    "titanic_no_nan['deck'] = titanic_no_nan['deck'].map(lambda x:  0 if x == 'A' else 1 if x == 'B' else 2 if x == 'C' else 3 if x == 'D' else 4 if x == 'E' else 5 if x == 'F' else 6)\n",
    "# 승선 도시 인코딩: 'Cherbourg'는 1, 'Queenstown'은 2, 'Southampton'은 3, 결측값은 0으로 매핑\n",
    "titanic_no_nan['embark_town'] = titanic_no_nan['embark_town'].map(lambda x: 1 if x == 'Cherbourg' else 2 if x == 'Queenstown' else 3 if x == 'Southampton' else 0)\n",
    "# 혼자 탑승 여부 인코딩: 'False'는 0, 'True'는 1로 매핑\n",
    "titanic_no_nan['alone'] = titanic_no_nan['alone'].map(lambda x: 0 if x == 'False' else 1)\n",
    "\n",
    "#head함수에서 class의 데이터 타입이 category인것으로 확인\n",
    "#타입 통일을 위해서 astype으로 int 적용해도 category로 나옴\n",
    "#str을 적용해서 int64로 바뀐것 확인\n",
    "titanic_no_nan['class'].astype(str)\n",
    "\n",
    "# 인코딩된 데이터프레임을 titanic_enco 변수에 저장\n",
    "titanic_enco = titanic_no_nan "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8de533d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    0\n",
      "Name: sex, dtype: int64\n",
      "0    1\n",
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "4    1\n",
      "Name: alive, dtype: int64\n",
      "0    2\n",
      "1    0\n",
      "2    2\n",
      "3    2\n",
      "4    2\n",
      "Name: embarked, dtype: int64\n",
      "0    2\n",
      "1    0\n",
      "2    2\n",
      "3    0\n",
      "4    2\n",
      "Name: class, dtype: category\n",
      "Categories (3, int64): [0, 1, 2]\n",
      "0    0\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    0\n",
      "Name: who, dtype: int64\n",
      "0    1\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    1\n",
      "Name: adult_male, dtype: int64\n",
      "0    6\n",
      "1    2\n",
      "2    6\n",
      "3    2\n",
      "4    6\n",
      "Name: deck, dtype: int64\n",
      "0    3\n",
      "1    1\n",
      "2    3\n",
      "3    3\n",
      "4    3\n",
      "Name: embark_town, dtype: int64\n",
      "0    1\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    1\n",
      "Name: alone, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#인코딩이 잘 이루어 졌는지 확인하기 위해 컬럼별로 head함수 적용\n",
    "print(titanic_enco['sex'].head())\n",
    "print(titanic_enco['alive'].head())\n",
    "print(titanic_enco['embarked'].head())\n",
    "print(titanic_enco['class'].head())\n",
    "print(titanic_enco['who'].head())\n",
    "print(titanic_enco['adult_male'].head())\n",
    "print(titanic_enco['deck'].head())\n",
    "print(titanic_enco['embark_town'].head())\n",
    "print(titanic_enco['alone'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "26991e78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    2\n",
      "1    2\n",
      "2    1\n",
      "3    2\n",
      "4    1\n",
      "Name: family_size, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#형제,자매, 부부, 부모, 자식의 수는 가족family_size로 묶을 수 있어 데이터 관리에 용이함\n",
    "#본인을 포함 시켜야 하니 +1\n",
    "titanic_enco['family_size'] = titanic_enco['sibsp'] + titanic_enco['parch'] + 1\n",
    "\n",
    "print(titanic_enco['family_size'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "631146f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: age\n",
      "       age\n",
      "7     2.00\n",
      "11   58.00\n",
      "15   55.00\n",
      "16    2.00\n",
      "33   66.00\n",
      "..     ...\n",
      "827   1.00\n",
      "829  62.00\n",
      "831   0.83\n",
      "851  74.00\n",
      "879  56.00\n",
      "\n",
      "[66 rows x 1 columns]\n",
      "\n",
      "Column: sibsp\n",
      "     sibsp\n",
      "7        3\n",
      "16       4\n",
      "24       3\n",
      "27       3\n",
      "50       4\n",
      "59       5\n",
      "63       3\n",
      "68       4\n",
      "71       5\n",
      "85       3\n",
      "88       3\n",
      "119      4\n",
      "159      8\n",
      "164      4\n",
      "171      4\n",
      "176      3\n",
      "180      8\n",
      "182      4\n",
      "201      8\n",
      "229      3\n",
      "233      4\n",
      "261      4\n",
      "266      4\n",
      "278      4\n",
      "324      8\n",
      "341      3\n",
      "374      3\n",
      "386      5\n",
      "409      3\n",
      "480      5\n",
      "485      3\n",
      "541      4\n",
      "542      4\n",
      "634      3\n",
      "642      3\n",
      "683      5\n",
      "686      4\n",
      "726      3\n",
      "787      4\n",
      "792      8\n",
      "813      4\n",
      "819      3\n",
      "824      4\n",
      "846      8\n",
      "850      4\n",
      "863      8\n",
      "\n",
      "Column: parch\n",
      "     parch\n",
      "7        1\n",
      "8        2\n",
      "10       1\n",
      "13       5\n",
      "16       1\n",
      "..     ...\n",
      "871      1\n",
      "879      1\n",
      "880      1\n",
      "885      5\n",
      "888      2\n",
      "\n",
      "[213 rows x 1 columns]\n",
      "\n",
      "Column: fare\n",
      "         fare\n",
      "1     71.2833\n",
      "27   263.0000\n",
      "31   146.5208\n",
      "34    82.1708\n",
      "52    76.7292\n",
      "..        ...\n",
      "846   69.5500\n",
      "849   89.1042\n",
      "856  164.8667\n",
      "863   69.5500\n",
      "879   83.1583\n",
      "\n",
      "[116 rows x 1 columns]\n",
      "\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#이상치를 확인 했지만 특별히 높은 값이 없어 그대로 진행\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#모든 열의 이상치 확인\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#25%보다 작거나 75%보다 크면 이상치로 처리\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#quantile로 이상치의 기준점 설정\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m column \u001b[38;5;129;01min\u001b[39;00m titanic_enco\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m----> 7\u001b[0m     Q1 \u001b[38;5;241m=\u001b[39m titanic_enco[column]\u001b[38;5;241m.\u001b[39mquantile(\u001b[38;5;241m0.25\u001b[39m)\n\u001b[0;32m      8\u001b[0m     Q3 \u001b[38;5;241m=\u001b[39m titanic_enco[column]\u001b[38;5;241m.\u001b[39mquantile(\u001b[38;5;241m0.75\u001b[39m)\n\u001b[0;32m      9\u001b[0m     IQR \u001b[38;5;241m=\u001b[39m Q3 \u001b[38;5;241m-\u001b[39m Q1\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:2887\u001b[0m, in \u001b[0;36mSeries.quantile\u001b[1;34m(self, q, interpolation)\u001b[0m\n\u001b[0;32m   2883\u001b[0m \u001b[38;5;66;03m# We dispatch to DataFrame so that core.internals only has to worry\u001b[39;00m\n\u001b[0;32m   2884\u001b[0m \u001b[38;5;66;03m#  about 2D cases.\u001b[39;00m\n\u001b[0;32m   2885\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[1;32m-> 2887\u001b[0m result \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mquantile(q\u001b[38;5;241m=\u001b[39mq, interpolation\u001b[38;5;241m=\u001b[39minterpolation, numeric_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   2888\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m   2889\u001b[0m     result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:12146\u001b[0m, in \u001b[0;36mDataFrame.quantile\u001b[1;34m(self, q, axis, numeric_only, interpolation, method)\u001b[0m\n\u001b[0;32m  12140\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis_number(axis)\n\u001b[0;32m  12142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_list_like(q):\n\u001b[0;32m  12143\u001b[0m     \u001b[38;5;66;03m# BlockManager.quantile expects listlike, so we wrap and unwrap here\u001b[39;00m\n\u001b[0;32m  12144\u001b[0m     \u001b[38;5;66;03m# error: List item 0 has incompatible type \"float | ExtensionArray |\u001b[39;00m\n\u001b[0;32m  12145\u001b[0m     \u001b[38;5;66;03m# ndarray[Any, Any] | Index | Series | Sequence[float]\"; expected \"float\"\u001b[39;00m\n\u001b[1;32m> 12146\u001b[0m     res_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquantile(\n\u001b[0;32m  12147\u001b[0m         [q],  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[0;32m  12148\u001b[0m         axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m  12149\u001b[0m         numeric_only\u001b[38;5;241m=\u001b[39mnumeric_only,\n\u001b[0;32m  12150\u001b[0m         interpolation\u001b[38;5;241m=\u001b[39minterpolation,\n\u001b[0;32m  12151\u001b[0m         method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m  12152\u001b[0m     )\n\u001b[0;32m  12153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msingle\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m  12154\u001b[0m         res \u001b[38;5;241m=\u001b[39m res_df\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:12191\u001b[0m, in \u001b[0;36mDataFrame.quantile\u001b[1;34m(self, q, axis, numeric_only, interpolation, method)\u001b[0m\n\u001b[0;32m  12187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m  12188\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid method: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Method must be in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalid_method\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m  12189\u001b[0m     )\n\u001b[0;32m  12190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msingle\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m> 12191\u001b[0m     res \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mquantile(qs\u001b[38;5;241m=\u001b[39mq, interpolation\u001b[38;5;241m=\u001b[39minterpolation)\n\u001b[0;32m  12192\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m  12193\u001b[0m     valid_interpolation \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnearest\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlower\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhigher\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1549\u001b[0m, in \u001b[0;36mBlockManager.quantile\u001b[1;34m(self, qs, interpolation)\u001b[0m\n\u001b[0;32m   1545\u001b[0m new_axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n\u001b[0;32m   1546\u001b[0m new_axes[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m Index(qs, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m   1548\u001b[0m blocks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m-> 1549\u001b[0m     blk\u001b[38;5;241m.\u001b[39mquantile(qs\u001b[38;5;241m=\u001b[39mqs, interpolation\u001b[38;5;241m=\u001b[39minterpolation) \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks\n\u001b[0;32m   1550\u001b[0m ]\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)(blocks, new_axes)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:1891\u001b[0m, in \u001b[0;36mBlock.quantile\u001b[1;34m(self, qs, interpolation)\u001b[0m\n\u001b[0;32m   1888\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m   1889\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m is_list_like(qs)  \u001b[38;5;66;03m# caller is responsible for this\u001b[39;00m\n\u001b[1;32m-> 1891\u001b[0m result \u001b[38;5;241m=\u001b[39m quantile_compat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues, np\u001b[38;5;241m.\u001b[39masarray(qs\u001b[38;5;241m.\u001b[39m_values), interpolation)\n\u001b[0;32m   1892\u001b[0m \u001b[38;5;66;03m# ensure_block_shape needed for cases where we start with EA and result\u001b[39;00m\n\u001b[0;32m   1893\u001b[0m \u001b[38;5;66;03m#  is ndarray, e.g. IntegerArray, SparseArray\u001b[39;00m\n\u001b[0;32m   1894\u001b[0m result \u001b[38;5;241m=\u001b[39m ensure_block_shape(result, ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\array_algos\\quantile.py:41\u001b[0m, in \u001b[0;36mquantile_compat\u001b[1;34m(values, qs, interpolation)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m quantile_with_mask(values, mask, fill_value, qs, interpolation)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m values\u001b[38;5;241m.\u001b[39m_quantile(qs, interpolation)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\_mixins.py:519\u001b[0m, in \u001b[0;36mNDArrayBackedExtensionArray._quantile\u001b[1;34m(self, qs, interpolation)\u001b[0m\n\u001b[0;32m    515\u001b[0m fill_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_fill_value\n\u001b[0;32m    517\u001b[0m res_values \u001b[38;5;241m=\u001b[39m quantile_with_mask(arr, mask, fill_value, qs, interpolation)\n\u001b[1;32m--> 519\u001b[0m res_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cast_quantile_result(res_values)\n\u001b[0;32m    520\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_from_backing_data(res_values)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\categorical.py:2480\u001b[0m, in \u001b[0;36mCategorical._cast_quantile_result\u001b[1;34m(self, res_values)\u001b[0m\n\u001b[0;32m   2478\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_cast_quantile_result\u001b[39m(\u001b[38;5;28mself\u001b[39m, res_values: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m   2479\u001b[0m     \u001b[38;5;66;03m# make sure we have correct itemsize for resulting codes\u001b[39;00m\n\u001b[1;32m-> 2480\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m res_values\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ndarray\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m   2481\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res_values\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#이상치를 확인 했지만 특별히 높은 값이 없어 그대로 진행\n",
    "#모든 열의 이상치 확인\n",
    "\n",
    "#25%보다 작거나 75%보다 크면 이상치로 처리\n",
    "#quantile로 이상치의 기준점 설정\n",
    "for column in titanic_enco.columns:\n",
    "    Q1 = titanic_enco[column].quantile(0.25)\n",
    "    Q3 = titanic_enco[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # 이상치 범위 설정\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    #여기서 |는 OR의 논리연산자\n",
    "    #C열의 값중 lower_bound보다 작거나 upper_bound보다 큰 수를 이상치로 간주해 outliers에 저장해서 확인\n",
    "    outliers = titanic_enco[(titanic_enco[column] < lower_bound) | (titanic_enco[column] > upper_bound)]\n",
    "\n",
    "    # 이상치가 있을 경우 출력\n",
    "    # outliers 데이터프레임이 비어 있지 않은지 확인하는 조건문\n",
    "    # empty 속성은 데이터프레임이 비어있으면 True, 데이터가 하나라도 있으면 False를 반환\n",
    "    if not outliers.empty:\n",
    "        print(f\"Column: {column}\")\n",
    "        print(outliers[[column]])  # 해당 열의 이상치만 출력\n",
    "        print()  # 줄바꿈\n",
    "        \n",
    "# 값이 출력 되었지만 양이 너무 적어보여 GPT에 제대로 작동 한지 검증해서 마지막 열에만 적용 된것을 확인\n",
    "# 조건문을 활용한 출력 값 코드 이해 X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f433ef15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110\n"
     ]
    }
   ],
   "source": [
    "# 중복값 확인\n",
    "#모델 성능 개선을 위해 중복값 존재하는지 확인 후 제거\n",
    "print(titanic_enco.duplicated().sum())\n",
    "\n",
    "#데이터 중복값 제거 후 titanic_no_duplicates 변수에 저장\n",
    "\n",
    "titanic_no_duplicates = titanic_enco.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5f0c315",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7898089171974523\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.84      0.82        89\n",
      "           1       0.78      0.72      0.75        68\n",
      "\n",
      "    accuracy                           0.79       157\n",
      "   macro avg       0.79      0.78      0.78       157\n",
      "weighted avg       0.79      0.79      0.79       157\n",
      "\n",
      "Confusion Matrix:\n",
      "[[75 14]\n",
      " [19 49]]\n"
     ]
    }
   ],
   "source": [
    "#로지스틱 회귀 모델\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "data = titanic_no_duplicates\n",
    "\n",
    "data = data[['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked', 'family_size']].dropna()\n",
    "\n",
    "X = data.drop('survived', axis=1)\n",
    "y = data['survived']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 데이터 스케일링\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# 모델 생성 및 학습\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 예측\n",
    "# model.predict(X_test) 학습된 모델을 사용하여 테스트데이터 X_test에 대한 예측값을 계산.예측 결과는 y_pred에 저장\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"Classification Report:\\n{classification_report(y_test, y_pred)}\")\n",
    "print(f\"Confusion Matrix:\\n{confusion_matrix(y_test, y_pred)}\")\n",
    "# accuracy_score(y_test, y_pred) 실제값 y_test와 예측 값 y_pred를 비교하여 정확도 계산\n",
    "# classification_report(y_test, y_pred) 모델의 성능을 요약하여 출력(정밀도, 재현율,F1점수, 지원)\n",
    "# confusion_matrix(y_test, y_pred) 혼동 행렬을 생성하여 모델의 성능을 시각적으로 나타낸다.\n",
    "#Precision (정밀도): 모델이 양성으로 예측한 것 중 실제 양성의 비율입니다.\n",
    "#Recall (재현율): 실제 양성 샘플 중에서 모델이 올바르게 예측한 비율입니다.\n",
    "#F1-Score: 정밀도와 재현율의 조화 평균으로, 두 지표의 균형을 잡아줍니다.(높을 수 록 성능이 균형있게 수행 된 것이다.)\n",
    "#평균 Avg 나온 숫자들이 비슷할 수록 좋다.\n",
    "# 혼동행렬: TP, FN, FP, TN\n",
    "#TP (True Positives): 올바르게 긍정으로 예측된 샘플 수\n",
    "#FP (False Positives): 잘못 긍정으로 예측된 샘플 수\n",
    "#TN (True Negatives): 올바르게 부정으로 예측된 샘플 수\n",
    "#FN (False Negatives): 잘못 부정으로 예측된 샘플 수\n",
    "\n",
    "#정확도는 67%\n",
    "#Classification Report의 차이가 크지 않은것으로 봐선 학습은 잘 진행 된것으로 보인다\n",
    "#recall이 0.56으로 낮아 긍정 클래스를 잘 놓치고 있다는 점이 우려 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "24de2b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "랜덤 포레스트 모델의 MSE: 0.17056965569709837\n",
      "훈련 데이터에 대한 MSE: 0.09365911883179091\n",
      "Accuracy: 0.6878980891719745\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.74      0.73        89\n",
      "           1       0.65      0.62      0.63        68\n",
      "\n",
      "    accuracy                           0.69       157\n",
      "   macro avg       0.68      0.68      0.68       157\n",
      "weighted avg       0.69      0.69      0.69       157\n",
      "\n",
      "Confusion Matrix:\n",
      "[[66 23]\n",
      " [26 42]]\n"
     ]
    }
   ],
   "source": [
    "#랜덤 포레스트 모델\n",
    "\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "data = titanic_no_duplicates\n",
    "\n",
    "data = data[['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked', 'family_size']].dropna()\n",
    "\n",
    "X = data.drop('survived', axis=1)\n",
    "y = data['survived']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 데이터 스케일링\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# 모델 학습\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 예측\n",
    "y_pred_rf = rf_model.predict(X_test_scaled)\n",
    "\n",
    "# 평가\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "print(f'랜덤 포레스트 모델의 MSE: {mse_rf}')\n",
    "\n",
    "# 훈련 데이터에서 예측\n",
    "y_train_pred = xgb_model.predict(X_train_scaled)\n",
    "\n",
    "# 훈련 데이터의 MSE 계산\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "print(f'훈련 데이터에 대한 MSE: {mse_train}')\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"Classification Report:\\n{classification_report(y_test, y_pred)}\")\n",
    "print(f\"Confusion Matrix:\\n{confusion_matrix(y_test, y_pred)}\")\n",
    "\n",
    "#MSE는 회귀모델에 적합한 평가모델이기 때문에 좋지 못한 성능으로 나온다.\n",
    "# 0 또는 1로 나뉘는 이진 분류는, MSE가 0.25는 예측이 실제 값에서 평균적으로 0.5 (반쯤 맞고 반쯤 틀림) 정도\n",
    "#과적합: 훈련 데이터에서 너무 성능이 좋고 테스트 데이터에서 성능이 떨어진다면 과적합의 징후\n",
    "#0.06 정도 차이로 과적합은 없는듯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9b67d14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost 모델의 MSE: 0.18471337579617833\n",
      "훈련 데이터에 대한 MSE: 0.125\n",
      "Accuracy: 0.8152866242038217\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.93      0.85        89\n",
      "           1       0.88      0.66      0.76        68\n",
      "\n",
      "    accuracy                           0.82       157\n",
      "   macro avg       0.83      0.80      0.80       157\n",
      "weighted avg       0.83      0.82      0.81       157\n",
      "\n",
      "Confusion Matrix:\n",
      "[[83  6]\n",
      " [23 45]]\n"
     ]
    }
   ],
   "source": [
    "#XGBoost 모델\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "data = titanic_no_duplicates\n",
    "\n",
    "data = data[['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked', 'family_size']].dropna()\n",
    "\n",
    "X = data.drop('survived', axis=1)\n",
    "y = data['survived']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 데이터 스케일링\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "\n",
    "# 모델 학습\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 예측\n",
    "y_pred_xgb = xgb_model.predict(X_test_scaled)\n",
    "\n",
    "# 평가\n",
    "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
    "print(f'XGBoost 모델의 MSE: {mse_xgb}')\n",
    "\n",
    "# 훈련 데이터에서 예측\n",
    "y_train_pred = xgb_model.predict(X_train_scaled)\n",
    "\n",
    "# 훈련 데이터의 MSE 계산\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "print(f'훈련 데이터에 대한 MSE: {mse_train}')\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_xgb)}\")\n",
    "print(f\"Classification Report:\\n{classification_report(y_test, y_pred_xgb)}\")\n",
    "print(f\"Confusion Matrix:\\n{confusion_matrix(y_test, y_pred_xgb)}\")\n",
    "\n",
    "#과적합: 훈련 데이터에서 너무 성능이 좋고 테스트 데이터에서 성능이 떨어진다면 과적합의 징후\n",
    "#0.06 정도 차이로 과적합은 없는듯\n",
    "# 0 또는 1로 나뉘는 이진 분류는, MSE가 0.25는 예측이 실제 값에서 평균적으로 0.5 (반쯤 맞고 반쯤 틀림) 정도\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a11e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 오류코드입니다 실행 시키지 마세요\n",
    "\n",
    "titanic_no_nan['sex'] = titanic_no_nan['sex'].map({'male': 0, 'female': 1})\n",
    "titanic_no_nan['alive'] = titanic_no_nan['alive'].map({'no': 1, 'yes': 0})\n",
    "titanic_no_nan['embarked'] = titanic_no_nan['embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
    "titanic_no_nan['class'] = titanic_no_nan['class'].map({'First': 0, 'Second': 1, 'Third': 2})\n",
    "titanic_no_nan['who'] = titanic_no_nan['who'].map({'man': 0, 'woman': 1})\n",
    "titanic_no_nan['adult_male'] = titanic_no_nan['adult_male'].map({'True': 1, 'False': 0})\n",
    "titanic_no_nan['deck'] = titanic_no_nan['deck'].map({'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7, 'nan': 0})\n",
    "titanic_no_nan['embark_town'] = titanic_no_nan['embark_town'].map({'Cherbourg': 1, 'Queenstown': 2, 'Southampton': 3, 'Unknown': 0})\n",
    "titanic_no_nan['alone'] = titanic_no_nan['alone'].map({'False': 0, 'True': 1})\n",
    "\n",
    "# map만 사용하였을때 nan값으로 처리 되어서 lambda함수를 같이 사용해야 된다는 피드백 받음\n",
    "# 그래서 lambda함수를 사용하면 왜 되는건지 찾아봄\n",
    "# map만 사용했을 땐 사전의 키와 값을 직접적으로 연결하여 변환하기에 간단한 매핑 가능\n",
    "# lambda함수를 사용하게 되면 복잡한 변환 로직 사용 가능하며 어떤 형식이든 유연히 처리\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1a897439",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.789809</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.720588</td>\n",
       "      <td>0.748092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>0.751592</td>\n",
       "      <td>0.754386</td>\n",
       "      <td>0.632353</td>\n",
       "      <td>0.688000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.789809</td>\n",
       "      <td>0.796610</td>\n",
       "      <td>0.691176</td>\n",
       "      <td>0.740157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Support Vector Machine</th>\n",
       "      <td>0.649682</td>\n",
       "      <td>0.709677</td>\n",
       "      <td>0.323529</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>K-Nearest Neighbors</th>\n",
       "      <td>0.687898</td>\n",
       "      <td>0.646154</td>\n",
       "      <td>0.617647</td>\n",
       "      <td>0.631579</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Accuracy  Precision    Recall  F1 Score\n",
       "Logistic Regression     0.789809   0.777778  0.720588  0.748092\n",
       "Decision Tree           0.751592   0.754386  0.632353  0.688000\n",
       "Random Forest           0.789809   0.796610  0.691176  0.740157\n",
       "Support Vector Machine  0.649682   0.709677  0.323529  0.444444\n",
       "K-Nearest Neighbors     0.687898   0.646154  0.617647  0.631579"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "data = titanic_no_duplicates\n",
    "\n",
    "data = data[['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked', 'family_size']].dropna()\n",
    "\n",
    "X = data.drop('survived', axis=1)\n",
    "y = data['survived']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# 모델 정의\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=200),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Support Vector Machine': SVC(),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "# 모델 학습 및 평가\n",
    "results = {}\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    results[model_name] = {\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred),\n",
    "        'Recall': recall_score(y_test, y_pred),\n",
    "        'F1 Score': f1_score(y_test, y_pred)\n",
    "    }\n",
    "\n",
    "results_df = pd.DataFrame(results).T\n",
    "\n",
    "\n",
    "results_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "53762d84",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of binary and continuous targets",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 47\u001b[0m\n\u001b[0;32m     43\u001b[0m     model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     44\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     46\u001b[0m     results[model_name] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m---> 47\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m: accuracy_score(y_test, y_pred),\n\u001b[0;32m     48\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrecision\u001b[39m\u001b[38;5;124m'\u001b[39m: precision_score(y_test, y_pred),\n\u001b[0;32m     49\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRecall\u001b[39m\u001b[38;5;124m'\u001b[39m: recall_score(y_test, y_pred),\n\u001b[0;32m     50\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF1 Score\u001b[39m\u001b[38;5;124m'\u001b[39m: f1_score(y_test, y_pred),\n\u001b[0;32m     51\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRandom Forest\u001b[39m\u001b[38;5;124m'\u001b[39m: RandomForestClassifier(y_test, y_pred)\n\u001b[0;32m     52\u001b[0m     }\n\u001b[0;32m     54\u001b[0m results_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(results)\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m     57\u001b[0m results_df\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:231\u001b[0m, in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    229\u001b[0m xp, _, device \u001b[38;5;241m=\u001b[39m get_namespace_and_device(y_true, y_pred, sample_weight)\n\u001b[0;32m    230\u001b[0m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[1;32m--> 231\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[0;32m    232\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:112\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m    109\u001b[0m     y_type \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_type) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 112\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    113\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassification metrics can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt handle a mix of \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m targets\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    114\u001b[0m             type_true, type_pred\n\u001b[0;32m    115\u001b[0m         )\n\u001b[0;32m    116\u001b[0m     )\n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m# We can't have more than one value on y_type => The set is no more needed\u001b[39;00m\n\u001b[0;32m    119\u001b[0m y_type \u001b[38;5;241m=\u001b[39m y_type\u001b[38;5;241m.\u001b[39mpop()\n",
      "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of binary and continuous targets"
     ]
    }
   ],
   "source": [
    "#랜덤 포레스트 모델\n",
    "\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "data = titanic_no_duplicates\n",
    "\n",
    "data = data[['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked', 'family_size']].dropna()\n",
    "\n",
    "X = data.drop('survived', axis=1)\n",
    "y = data['survived']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# 모델 학습\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 예측\n",
    "y_pred_rf = rf_model.predict(X_test_scaled)\n",
    "\n",
    "\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=200),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Support Vector Machine': SVC(),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "    'Random Forest' : RandomForestClassifier()\n",
    "}\n",
    "\n",
    "# 모델 학습 및 평가\n",
    "results = {}\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    results[model_name] = {\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred),\n",
    "        'Recall': recall_score(y_test, y_pred),\n",
    "        'F1 Score': f1_score(y_test, y_pred),\n",
    "        'Random Forest': RandomForestClassifier(y_test, y_pred)\n",
    "    }\n",
    "\n",
    "results_df = pd.DataFrame(results).T\n",
    "\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c416306",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
